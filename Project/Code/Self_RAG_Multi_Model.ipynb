{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7Awq1xFStwQ"
   },
   "source": [
    "# Cross-Lingual Self-RAG with Multi-Model Support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hItBZGSStwR"
   },
   "source": [
    "## 1. Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRbS7JpxStwR"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch wikipedia-api googletrans==4.0.0-rc1 sentence-transformers\n",
    "!pip install -q numpy pandas tqdm colorama huggingface-hub accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10979,
     "status": "ok",
     "timestamp": 1765970139001,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "xZJ3r9TCStwS",
    "outputId": "7a52bacf-aeeb-48a3-83b6-8ca4cb5a146b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "Memory: 15.83 GB\n",
      "\n",
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from enum import Enum\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style, init\n",
    "\n",
    "# Translation and NLP\n",
    "from googletrans import Translator\n",
    "import wikipediaapi\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline\n",
    ")\n",
    "import torch\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Initialize colorama for colored output\n",
    "init(autoreset=True)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\nAll packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj-JrvSEStwS"
   },
   "source": [
    "## 2. Hugging Face Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 140,
     "status": "ok",
     "timestamp": 1765970139147,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "mlKrtRlUStwS",
    "outputId": "452306e7-1769-44d9-ed21-6b85c94cce0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face\n",
    "try:\n",
    "    login(os.getenv(\"HF_TOKEN\"))\n",
    "    print(\"✅ Successfully logged in to Hugging Face!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Warning: Could not login to Hugging Face: {e}\")\n",
    "    print(\"You may need to authenticate manually or some models might not be accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-muzBICTStwS"
   },
   "source": [
    "## 3. Configuration and Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GU5Qpef-StwT"
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration for the Cross-Lingual Self-RAG system with Multi-Model Support\"\"\"\n",
    "    # Language settings\n",
    "    supported_languages: List[str] = None\n",
    "    language_codes: Dict[str, str] = None\n",
    "\n",
    "    # Retrieval settings\n",
    "    wikipedia_top_k: int = 3\n",
    "    max_passage_length: int = 500\n",
    "\n",
    "    # Model settings - will be set dynamically based on input\n",
    "    llm_model: str = \"google/gemma-2b-it\"  # Default, will be overridden\n",
    "    model_type: str = \"gemma\"  # Default, will be overridden\n",
    "    llm_temperature: float = 0.1\n",
    "    llm_max_new_tokens: int = 512  # Maximum new tokens to generate\n",
    "    llm_top_p: float = 0.95\n",
    "    llm_top_k: int = 50\n",
    "    llm_repetition_penalty: float = 1.1\n",
    "\n",
    "    # Memory optimization settings\n",
    "    load_in_8bit: bool = False  # Set to True if you have memory constraints\n",
    "    load_in_4bit: bool = False  # Even more memory efficient\n",
    "    device_map: str = \"auto\"  # Automatically map model to available devices\n",
    "    torch_dtype: str = \"float16\"  # Use float16 for memory efficiency\n",
    "\n",
    "    # Output settings\n",
    "    verbose: bool = False\n",
    "    save_intermediate: bool = True\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.supported_languages = ['tamil', 'hindi', 'kannada', 'marathi', 'english']\n",
    "        self.language_codes = {\n",
    "            'tamil': 'ta',\n",
    "            'hindi': 'hi',\n",
    "            'kannada': 'kn',\n",
    "            'marathi': 'mr',\n",
    "            'english': 'en'\n",
    "        }\n",
    "\n",
    "# Data classes for structured outputs\n",
    "class SupportJudgment(Enum):\n",
    "    SUPPORTED = \"supported\"\n",
    "    PARTIALLY_SUPPORTED = \"partially_supported\"\n",
    "    NOT_SUPPORTED = \"not_supported\"\n",
    "\n",
    "@dataclass\n",
    "class CritiqueResult:\n",
    "    judgment: SupportJudgment\n",
    "    rationale: str\n",
    "    revised_answer: str\n",
    "    confidence: float\n",
    "\n",
    "@dataclass\n",
    "class SelfRAGOutput:\n",
    "    \"\"\"Final output structure as specified in Step 9\"\"\"\n",
    "    final_answer_indic: str  # Final answer in Indic language\n",
    "    judgment_label: str  # supported / partial / not supported\n",
    "    support_confidence: float  # Confidence score [0, 1]\n",
    "    english_rationale: str  # Rationale for auditability\n",
    "\n",
    "    # Additional fields for debugging\n",
    "    initial_answer: str = None\n",
    "    retrieved_context: str = None\n",
    "    regenerated_answer: str = None\n",
    "    english_answer: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGOo_tDNStwT"
   },
   "source": [
    "## 4. Wikipedia Retrieval Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pe_jIjfpStwU"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import html\n",
    "\n",
    "class WikipediaRetriever:\n",
    "    \"\"\"On-the-fly Wikipedia retrieval via MediaWiki API (multilingual)\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def resolve_language_key(self, language: str) -> str:\n",
    "        language = language.strip().lower()\n",
    "        if language in self.config.language_codes:\n",
    "            return language\n",
    "        for lang_key in self.config.language_codes:\n",
    "            if lang_key.lower() == language:\n",
    "                return lang_key\n",
    "        return language\n",
    "\n",
    "    def normalize_token(self, token: str, language: str) -> str:\n",
    "        return re.sub(r\"[^\\w\\u0B80-\\u0BFF\\u0900-\\u097F\\u0C80-\\u0CFF\\u0964-\\u0965]\", \"\", token).strip()\n",
    "\n",
    "    def extract_keywords(self, question: str, language: str) -> List[str]:\n",
    "        stop_patterns = {\n",
    "            \"tamil\": [\"என்ன\", \"என்று\", \"எப்படி\", \"ஏன்\", \"எங்கே\", \"எப்போது\", \"யார்\"],\n",
    "            \"hindi\": [\"क्या\", \"कब\", \"कहाँ\", \"कैसे\", \"क्यों\", \"कौन\", \"है\", \"हैं\", \"के\", \"में\", \"का\"],\n",
    "            \"kannada\": [\"ಏನು\", \"ಯಾವಾಗ\", \"ಎಲ್ಲಿ\", \"ಹೇಗೆ\", \"ಏಕೆ\", \"ಯಾರು\"],\n",
    "            \"marathi\": [\"काय\", \"कधी\", \"कुठे\", \"कसे\", \"का\", \"कोण\"],\n",
    "            \"english\": [\"what\", \"when\", \"where\", \"how\", \"why\", \"who\", \"is\", \"are\", \"the\", \"a\", \"an\", \"of\"]\n",
    "        }\n",
    "\n",
    "        raw_words = question.split()\n",
    "        normalized_words = [self.normalize_token(w, language) for w in raw_words]\n",
    "\n",
    "        stop_words = stop_patterns.get(language, [])\n",
    "        stop_lower = {s.lower() for s in stop_words}\n",
    "\n",
    "        keywords = [w for w in normalized_words if w and w.lower() not in stop_lower]\n",
    "        return keywords[:5]\n",
    "\n",
    "    def mediawiki_search(self, language: str, query: str, limit: int) -> List[Dict]:\n",
    "        code = self.config.language_codes.get(language)\n",
    "        if not code:\n",
    "            return []\n",
    "\n",
    "        url = f\"https://{code}.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": query,\n",
    "            \"srlimit\": limit,\n",
    "            \"format\": \"json\",\n",
    "            \"utf8\": 1,\n",
    "        }\n",
    "\n",
    "        r = requests.get(\n",
    "            url, params=params, timeout=8,\n",
    "            headers={\"User-Agent\": \"CrossLingualRAG/1.0\"}\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        return data.get(\"query\", {}).get(\"search\", [])\n",
    "\n",
    "    def fetch_extracts(self, language: str, pageids: List[int], chars: int) -> Dict[int, str]:\n",
    "        code = self.config.language_codes.get(language)\n",
    "        if not code or not pageids:\n",
    "            return {}\n",
    "\n",
    "        url = f\"https://{code}.wikipedia.org/w/api.php\"\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"prop\": \"extracts|pageprops\",\n",
    "            \"exintro\": 1,\n",
    "            \"explaintext\": 1,\n",
    "            \"exchars\": chars,\n",
    "            \"pageids\": \"|\".join(str(pid) for pid in pageids),\n",
    "            \"format\": \"json\",\n",
    "            \"utf8\": 1,\n",
    "        }\n",
    "\n",
    "        r = requests.get(\n",
    "            url, params=params, timeout=8,\n",
    "            headers={\"User-Agent\": \"CrossLingualRAG/1.0\"}\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "\n",
    "        pages = r.json().get(\"query\", {}).get(\"pages\", {})\n",
    "        out: Dict[int, str] = {}\n",
    "\n",
    "        for pid_str, pdata in pages.items():\n",
    "            if isinstance(pdata, dict) and pdata.get(\"pageprops\", {}).get(\"disambiguation\") is not None:\n",
    "                continue\n",
    "            try:\n",
    "                out[int(pid_str)] = pdata.get(\"extract\", \"\") or \"\"\n",
    "            except Exception:\n",
    "                pass\n",
    "        return out\n",
    "\n",
    "    def build_query_variants(self, question: str, language: str) -> List[str]:\n",
    "        q_full = re.sub(\n",
    "            r\"\\s+\", \" \",\n",
    "            re.sub(r\"[^\\w\\s\\u0B80-\\u0BFF\\u0900-\\u097F\\u0C80-\\u0CFF]\", \" \", question)\n",
    "        ).strip()\n",
    "\n",
    "        keywords = self.extract_keywords(question, language)\n",
    "\n",
    "        variants: List[str] = []\n",
    "        if q_full:\n",
    "            variants.append(q_full)\n",
    "        if keywords:\n",
    "            variants.append(\" \".join(keywords))\n",
    "\n",
    "        for i in range(len(keywords) - 1):\n",
    "            variants.append(keywords[i] + \" \" + keywords[i + 1])\n",
    "\n",
    "        variants.extend(keywords)\n",
    "\n",
    "        seen = set()\n",
    "        uniq = []\n",
    "        for v in variants:\n",
    "            k = v.strip().lower()\n",
    "            if v.strip() and k not in seen:\n",
    "                seen.add(k)\n",
    "                uniq.append(v.strip())\n",
    "\n",
    "        return uniq[:10]\n",
    "\n",
    "    def search_wikipedia(self, question: str, language: str, top_k: Optional[int] = None) -> List[str]:\n",
    "        if top_k is None:\n",
    "            top_k = self.config.wikipedia_top_k\n",
    "\n",
    "        top_k = max(1, int(top_k))\n",
    "\n",
    "        language = self.resolve_language_key(language)\n",
    "        if language not in self.config.language_codes:\n",
    "            if getattr(self.config, \"verbose\", False):\n",
    "                print(f\"[WikipediaRetriever] Unknown language key: {language}. \"\n",
    "                      f\"Available: {list(self.config.language_codes.keys())}\")\n",
    "            return []\n",
    "\n",
    "        variants = self.build_query_variants(question, language)\n",
    "        if getattr(self.config, \"verbose\", False):\n",
    "            print(f\"[WikipediaRetriever] Query variants: {variants}\")\n",
    "\n",
    "        candidates = {}\n",
    "        try:\n",
    "            for q in variants:\n",
    "                hits = self.mediawiki_search(language, q, limit=max(10, top_k * 3))\n",
    "                for h in hits:\n",
    "                    pid = int(h.get(\"pageid\", 0))\n",
    "                    if not pid or pid in candidates:\n",
    "                        continue\n",
    "                    title = h.get(\"title\", \"\") or \"\"\n",
    "                    snippet = h.get(\"snippet\", \"\") or \"\"\n",
    "                    snippet = html.unescape(re.sub(r\"<[^>]+>\", \"\", snippet))\n",
    "                    candidates[pid] = (title, snippet)\n",
    "\n",
    "                if len(candidates) >= top_k * 10:\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            if getattr(self.config, \"verbose\", False):\n",
    "                print(f\"[WikipediaRetriever] Search error: {e}\")\n",
    "            return []\n",
    "\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        pid_list = list(candidates.keys())[:max(30, top_k * 10)]\n",
    "        try:\n",
    "            extracts = self.fetch_extracts(language, pid_list, chars=self.config.max_passage_length)\n",
    "        except Exception as e:\n",
    "            if getattr(self.config, \"verbose\", False):\n",
    "                print(f\"[WikipediaRetriever] Extract error: {e}\")\n",
    "            return []\n",
    "\n",
    "        q_tokens = re.findall(r\"\\w+\", question.lower())\n",
    "        q_tokens = {t for t in q_tokens if len(t) >= 3}\n",
    "\n",
    "        scored = []\n",
    "        for pid in pid_list:\n",
    "            title, snippet = candidates[pid]\n",
    "            extract = extracts.get(pid, \"\") or \"\"\n",
    "            title_l = title.lower()\n",
    "            snippet_l = snippet.lower()\n",
    "            extract_l = extract.lower()\n",
    "\n",
    "            title_hits = sum(1 for t in q_tokens if t in title_l)\n",
    "            snippet_hits = sum(1 for t in q_tokens if t in snippet_l)\n",
    "            extract_hits = sum(1 for t in q_tokens if t in extract_l)\n",
    "\n",
    "            score = 3 * title_hits + 2 * snippet_hits + 1 * extract_hits\n",
    "            scored.append((score, pid, title, extract))\n",
    "\n",
    "        scored.sort(reverse=True, key=lambda x: x[0])\n",
    "\n",
    "        passages = []\n",
    "        for score, pid, title, extract in scored[:top_k]:\n",
    "            extract = (extract or \"\").strip()\n",
    "            if extract:\n",
    "                passages.append(f\"[Source: {title}]\\n{extract[:self.config.max_passage_length]}\")\n",
    "\n",
    "        return passages\n",
    "\n",
    "    def retrieve(self, question: str, language: str, gold_context: Optional[str] = None) -> str:\n",
    "        contexts = []\n",
    "\n",
    "        if gold_context:\n",
    "            contexts.append(f\"[Gold Context]\\n{gold_context}\")\n",
    "\n",
    "        wiki_passages = self.search_wikipedia(question, language, top_k=self.config.wikipedia_top_k)\n",
    "        contexts.extend(wiki_passages)\n",
    "\n",
    "        combined = \"\\n\\n\".join(contexts)\n",
    "        return combined if combined else \"No relevant context found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJhQPJFAStwU"
   },
   "source": [
    "## 5. Translation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xIlZnOxStwV"
   },
   "outputs": [],
   "source": [
    "class TranslationModule:\n",
    "    \"\"\"Module for cross-lingual translation\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.translator = Translator()\n",
    "\n",
    "    def translate_to_english(self, text: str, source_lang: str) -> str:\n",
    "        \"\"\"Translate text to English\"\"\"\n",
    "        if not text or source_lang == 'english':\n",
    "            return text\n",
    "\n",
    "        try:\n",
    "            lang_code = self.config.language_codes.get(source_lang, source_lang)\n",
    "            result = self.translator.translate(text, src=lang_code, dest='en')\n",
    "            return result.text\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {e}\")\n",
    "            return text\n",
    "\n",
    "    def translate_from_english(self, text: str, target_lang: str) -> str:\n",
    "        \"\"\"Translate text from English to target language\"\"\"\n",
    "        if not text or target_lang == 'english':\n",
    "            return text\n",
    "\n",
    "        try:\n",
    "            lang_code = self.config.language_codes.get(target_lang, target_lang)\n",
    "            result = self.translator.translate(text, src='en', dest=lang_code)\n",
    "            return result.text\n",
    "        except Exception as e:\n",
    "            print(f\"Translation error: {e}\")\n",
    "            return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wg7AcJSdStwV"
   },
   "source": [
    "## 6. Multi-Model LLM Module (Gemma, Sarvam, Aya)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5NUaQjQhtbN"
   },
   "outputs": [],
   "source": [
    "import os, shelve, hashlib\n",
    "\n",
    "def _cache_key(question: str, language: str) -> str:\n",
    "    # stable key even if question is long\n",
    "    h = hashlib.sha256(f\"{language}||{question}\".encode(\"utf-8\")).hexdigest()\n",
    "    return h\n",
    "\n",
    "class RetrievalCache:\n",
    "    def __init__(self, cache_path: str):\n",
    "        os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n",
    "        self.cache_path = cache_path\n",
    "\n",
    "    def get(self, question: str, language: str):\n",
    "        key = _cache_key(question, language)\n",
    "        with shelve.open(self.cache_path) as db:\n",
    "            return db.get(key, None)\n",
    "\n",
    "    def set(self, question: str, language: str, context: str):\n",
    "        key = _cache_key(question, language)\n",
    "        with shelve.open(self.cache_path) as db:\n",
    "            db[key] = context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfGCp-G9StwV"
   },
   "outputs": [],
   "source": [
    "class MultiModelSelfRAG:\n",
    "    \"\"\"Multi-model Self-RAG module supporting Gemma, Sarvam, and Aya models\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.current_model_type = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the appropriate model based on config.model_type\"\"\"\n",
    "        model_type = self.config.model_type.lower()\n",
    "\n",
    "        # Model mapping\n",
    "        model_mapping = {\n",
    "            \"gemma\": \"google/gemma-2b-it\",\n",
    "            \"sarvam\": \"sarvamai/sarvam-1\",\n",
    "            \"aya\": \"CohereLabs/aya-101\"\n",
    "        }\n",
    "\n",
    "        if model_type not in model_mapping:\n",
    "            print(f\"Warning: Unknown model type {model_type}, using gemma as default\")\n",
    "            model_type = \"gemma\"\n",
    "\n",
    "        self.config.llm_model = model_mapping[model_type]\n",
    "        self.current_model_type = model_type\n",
    "\n",
    "        print(f\"Loading {model_type.upper()} model: {self.config.llm_model}\")\n",
    "\n",
    "        # Quantization configuration\n",
    "        bnb_config = None\n",
    "        if self.config.load_in_4bit:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_quant_type=\"nf4\"\n",
    "            )\n",
    "        elif self.config.load_in_8bit:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                bnb_8bit_compute_dtype=torch.float16\n",
    "            )\n",
    "\n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.config.llm_model,\n",
    "            use_auth_token=True\n",
    "        )\n",
    "\n",
    "        # Set padding token if not present\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        # Load model with appropriate dtype and device mapping\n",
    "        torch_dtype = torch.float16 if self.config.torch_dtype == \"float16\" else torch.float32\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config.llm_model,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device_map=self.config.device_map,\n",
    "            quantization_config=bnb_config,\n",
    "            use_auth_token=True,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        print(f\"{Fore.GREEN}✓ {model_type.upper()} Model loaded successfully!{Style.RESET_ALL}\")\n",
    "\n",
    "        # Print model info\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        print(f\"Model parameters: {total_params / 1e9:.2f}B\")\n",
    "\n",
    "    def _format_prompt(self, instruction: str, context: str = \"\") -> str:\n",
    "        \"\"\"Format prompt based on model type\"\"\"\n",
    "        if self.current_model_type == \"gemma\":\n",
    "            # Gemma-IT uses a specific format for instructions\n",
    "            if context:\n",
    "                prompt = f\"<start_of_turn>user\\n{context}\\n\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "            else:\n",
    "                prompt = f\"<start_of_turn>user\\n{instruction}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        elif self.current_model_type == \"sarvam\":\n",
    "            # Sarvam model format - adjust based on actual model requirements\n",
    "            if context:\n",
    "                prompt = f\"### Context:\\n{context}\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "            else:\n",
    "                prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
    "        elif self.current_model_type == \"aya\":\n",
    "            # Aya model format - adjust based on actual model requirements\n",
    "            if context:\n",
    "                prompt = f\"Context: {context}\\n\\nQuestion: {instruction}\\n\\nAnswer:\"\n",
    "            else:\n",
    "                prompt = f\"Question: {instruction}\\n\\nAnswer:\"\n",
    "        else:\n",
    "            # Default format\n",
    "            if context:\n",
    "                prompt = f\"{context}\\n\\n{instruction}\"\n",
    "            else:\n",
    "                prompt = instruction\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def _generate_response(self, prompt: str) -> str:\n",
    "        \"\"\"Generate response using the loaded model\"\"\"\n",
    "        try:\n",
    "            # Tokenize input\n",
    "            inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=2048,\n",
    "                padding=True\n",
    "            )\n",
    "\n",
    "            # Get the model's device\n",
    "            model_device = next(self.model.parameters()).device\n",
    "\n",
    "            # Move ALL inputs to the same device as the model\n",
    "            inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "            # Ensure attention_mask is present\n",
    "            if 'attention_mask' not in inputs:\n",
    "                inputs['attention_mask'] = torch.ones_like(inputs['input_ids'])\n",
    "\n",
    "            # Generate response\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.config.llm_max_new_tokens,\n",
    "                    temperature=self.config.llm_temperature,\n",
    "                    top_p=self.config.llm_top_p,\n",
    "                    top_k=self.config.llm_top_k,\n",
    "                    repetition_penalty=self.config.llm_repetition_penalty,\n",
    "                    do_sample=True if self.config.llm_temperature > 0 else False,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            # Decode response\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            return response.strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def regenerate_answer(self, question: str, initial_answer: str, context: str, language: str) -> str:\n",
    "        \"\"\"Step 3: Self-RAG Regeneration in source language\"\"\"\n",
    "        instruction = f\"\"\"You are a helpful assistant that answers questions based strictly on provided context.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Initial Answer: {initial_answer}\n",
    "\n",
    "Task: Re-answer the question using ONLY the information present in the context below.\n",
    "If Gold Context is present, slightly prefer it over Wikipedia-based context when answering.\n",
    "When using Wikipedia-based context, first compare it with the question and use it only if the context clearly supports it.\n",
    "If the required information is missing or unclear in the context, explicitly state that you cannot answer due to lack of context.\n",
    "Be factual and concise. Answer in {language}.\n",
    "\n",
    "Your answer:\"\"\"\n",
    "\n",
    "        formatted_prompt = self._format_prompt(instruction, context)\n",
    "        regenerated = self._generate_response(formatted_prompt)\n",
    "\n",
    "        if self.config.verbose:\n",
    "            print(f\"{Fore.CYAN}Regenerated answer in {language} using {self.current_model_type.upper()}{Style.RESET_ALL}\")\n",
    "\n",
    "        return regenerated\n",
    "\n",
    "    def critique_answer(self, question_en: str, context_en: str, answer_en: str):\n",
    "        \"\"\"\n",
    "        Cross-lingual critique.\n",
    "        Returns: (confidence: float, revised_answer: str)\n",
    "        \"\"\"\n",
    "\n",
    "        instruction = f\"\"\"You are a factual verification and revision system.\n",
    "\n",
    "    Question:\n",
    "    {question_en}\n",
    "\n",
    "    Current Answer:\n",
    "    {answer_en}\n",
    "\n",
    "    Context:\n",
    "    {context_en}\n",
    "\n",
    "    Return only:\n",
    "    A revised answer grounded ONLY in the Context, OR a clear refusal saying, I can't answer due to missing context.\n",
    "\n",
    "    Rules:\n",
    "    - Use ONLY information explicitly stated in the Context.\n",
    "    - Do NOT output JSON, markdown, labels, or explanations.\n",
    "    \"\"\"\n",
    "\n",
    "        formatted_prompt = self._format_prompt(instruction, context=\"\")\n",
    "        response = (self._generate_response(formatted_prompt) or \"\").strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up model from memory\"\"\"\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def regenerate_answer_batch(\n",
    "        self,\n",
    "        questions,\n",
    "        initial_answers,\n",
    "        contexts,\n",
    "        language,\n",
    "        batch_size=8\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Batched version of regenerate_answer for fast GPU inference\n",
    "        \"\"\"\n",
    "\n",
    "        assert len(questions) == len(initial_answers) == len(contexts)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # REQUIRED for generation\n",
    "        self.tokenizer.padding_side = \"left\"\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        for i in range(0, len(questions), batch_size):\n",
    "            batch_q = questions[i:i + batch_size]\n",
    "            batch_a = initial_answers[i:i + batch_size]\n",
    "            batch_c = contexts[i:i + batch_size]\n",
    "\n",
    "            prompts = []\n",
    "\n",
    "            for q, a, c in zip(batch_q, batch_a, batch_c):\n",
    "                instruction = f\"\"\"You are a helpful assistant that answers questions based strictly on provided context.\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "Initial Answer: {a}\n",
    "\n",
    "Task: Re-answer the question using ONLY the information present in the context below.\n",
    "If Gold Context is present, try to prefer it over Wikipedia-based context when answering.\n",
    "If the required information is missing or unclear, explicitly say so. Don't give one word answers.\n",
    "Answer in {language}.\n",
    "\n",
    "Your answer:\"\"\"\n",
    "\n",
    "                prompts.append(self._format_prompt(instruction, c))\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                prompts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=2048\n",
    "            )\n",
    "\n",
    "            model_device = next(self.model.parameters()).device\n",
    "            inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=self.config.llm_max_new_tokens,\n",
    "                    temperature=self.config.llm_temperature,\n",
    "                    top_p=self.config.llm_top_p,\n",
    "                    top_k=self.config.llm_top_k,\n",
    "                    repetition_penalty=self.config.llm_repetition_penalty,\n",
    "                    do_sample=self.config.llm_temperature > 0,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            decoded = self.tokenizer.batch_decode(\n",
    "                outputs[:, inputs[\"input_ids\"].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            results.extend([x.strip() for x in decoded])\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzMKxMHTStwV"
   },
   "source": [
    "## 7. Main Cross-Lingual Self-RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xjm1LxNCStwV"
   },
   "outputs": [],
   "source": [
    "class CrossLingualSelfRAG:\n",
    "    \"\"\"Main pipeline integrating all components with Multi-Model Support\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config = None):\n",
    "        self.config = config or Config()\n",
    "\n",
    "        # Initialize modules\n",
    "        print(\"Initializing Cross-Lingual Self-RAG modules...\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(\"1. Initializing Wikipedia Retriever...\")\n",
    "        self.retriever = WikipediaRetriever(self.config)\n",
    "\n",
    "        print(\"\\n2. Initializing Translation Module...\")\n",
    "        self.translator = TranslationModule(self.config)\n",
    "\n",
    "        print(f\"\\n3. Initializing {self.config.model_type.upper()} Model...\")\n",
    "        self.llm = MultiModelSelfRAG(self.config)\n",
    "\n",
    "        print(\"=\"*60)\n",
    "        print(f\"{Fore.GREEN}✓ All modules initialized successfully!{Style.RESET_ALL}\\n\")\n",
    "\n",
    "    def process(self,\n",
    "                question_L: str,\n",
    "                answer_L: str,\n",
    "                language_L: str,\n",
    "                context_L: Optional[str] = None) -> SelfRAGOutput:\n",
    "        \"\"\"\n",
    "        Main processing pipeline\n",
    "\n",
    "        Args:\n",
    "            question_L: Question in Indic language\n",
    "            answer_L: Initial answer in Indic language\n",
    "            language_L: Source language (tamil/hindi/kannada/marathi)\n",
    "            context_L: Optional gold context in Indic language\n",
    "\n",
    "        Returns:\n",
    "            SelfRAGOutput with all results\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"\\n{Fore.YELLOW}=== Processing Query in {language_L.upper()} ==={Style.RESET_ALL}\")\n",
    "\n",
    "        # Step 2: On-the-fly Wikipedia Retrieval\n",
    "        retrieved_context = self.retriever.retrieve(question_L, language_L, context_L)\n",
    "\n",
    "        # Step 3: Self-RAG Regeneration in source language\n",
    "        regenerated_answer_L = self.llm.regenerate_answer(\n",
    "            question_L, answer_L, retrieved_context, language_L\n",
    "        )\n",
    "\n",
    "        # Step 4: Cross-lingual projection to English\n",
    "        question_EN = self.translator.translate_to_english(question_L, language_L)\n",
    "        context_EN = self.translator.translate_to_english(retrieved_context, language_L)\n",
    "        regenerated_answer_EN = self.translator.translate_to_english(regenerated_answer_L, language_L)\n",
    "\n",
    "        # Step 5: Cross-lingual self-critique\n",
    "        revised_answer_EN = self.llm.critique_answer(question_EN, context_EN, regenerated_answer_EN)\n",
    "\n",
    "        # Step 7: Back-projection to source language\n",
    "        final_answer_L = self.translator.translate_from_english(\n",
    "            revised_answer_EN, language_L\n",
    "        )\n",
    "\n",
    "        # Step 8: Final output structure\n",
    "        output = SelfRAGOutput(\n",
    "            final_answer_indic=final_answer_L,\n",
    "            support_confidence=1,\n",
    "            initial_answer=answer_L,\n",
    "            retrieved_context=retrieved_context[:500],\n",
    "            regenerated_answer=regenerated_answer_L,\n",
    "            english_answer=revised_answer_EN,\n",
    "        )\n",
    "\n",
    "\n",
    "        print(f\"{Fore.GREEN}✓ Processing complete{Style.RESET_ALL}\")\n",
    "        return output\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up all modules\"\"\"\n",
    "        print(\"\\nCleaning up resources...\")\n",
    "        self.llm.cleanup()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        print(\"✓ Cleanup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuLrv6y41RpK"
   },
   "source": [
    "## 8. CSV Processing with Dynamic Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALggFqeI1RpK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def detect_model_and_language_from_path(input_csv_path):\n",
    "    \"\"\"\n",
    "    Detect model type and language from the CSV path.\n",
    "    Expected path structure: .../Results/Vanilla/{model}/{model}_results_{language}.csv\n",
    "    \"\"\"\n",
    "    path_parts = input_csv_path.replace(\"\\\\\", \"/\").split(\"/\")\n",
    "\n",
    "    # Try to find model name from path\n",
    "    model_type = \"gemma\"  # default\n",
    "    language = \"tamil\"  # default\n",
    "\n",
    "    # Look for model folder name\n",
    "    if \"sarvam\" in input_csv_path.lower():\n",
    "        model_type = \"sarvam\"\n",
    "    elif \"aya\" in input_csv_path.lower():\n",
    "        model_type = \"aya\"\n",
    "    elif \"gemma\" in input_csv_path.lower():\n",
    "        model_type = \"gemma\"\n",
    "\n",
    "    # Try to extract language from filename\n",
    "    filename = os.path.basename(input_csv_path).lower()\n",
    "    if \"ta\" in filename:\n",
    "        language = \"tamil\"\n",
    "    elif \"en\" in filename:\n",
    "        language = \"english\"\n",
    "    elif \"hi\" in filename:\n",
    "        language = \"hindi\"\n",
    "    elif \"ka\" in filename:\n",
    "        language = \"kannada\"\n",
    "    elif \"ma\" in filename:\n",
    "        language = \"marathi\"\n",
    "\n",
    "    return model_type, language\n",
    "\n",
    "# def run_selfrag_on_csv(input_csv_path, model_type=None, language=None):\n",
    "#     \"\"\"\n",
    "#     Run Self-RAG on a CSV file with dynamic model selection.\n",
    "\n",
    "#     Args:\n",
    "#         input_csv_path: path to a CSV under Results/Vanilla/...\n",
    "#         model_type: 'gemma', 'sarvam', or 'aya' (if None, will auto-detect)\n",
    "#         language: language of the content (if None, will auto-detect)\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Auto-detect model and language if not provided\n",
    "#     if model_type is None or language is None:\n",
    "#         detected_model, detected_language = detect_model_and_language_from_path(input_csv_path)\n",
    "#         if model_type is None:\n",
    "#             model_type = detected_model\n",
    "#         if language is None:\n",
    "#             language = detected_language\n",
    "\n",
    "#     print(f\"\\n\" + \"=\"*60)\n",
    "#     print(f\"Processing: {os.path.basename(input_csv_path)}\")\n",
    "#     print(f\"Model: {model_type.upper()}\")\n",
    "#     print(f\"Language: {language.upper()}\")\n",
    "#     print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "#     # Create config with the appropriate model\n",
    "#     config = Config()\n",
    "#     config.model_type = model_type\n",
    "\n",
    "#     # Adjust settings based on model size if needed\n",
    "#     if model_type == \"aya\":\n",
    "#         # Aya is typically larger, might need more memory optimization\n",
    "#         config.load_in_4bit = True  # Enable 4-bit quantization for larger models\n",
    "\n",
    "#     # Initialize pipeline with the selected model\n",
    "#     pipeline = CrossLingualSelfRAG(config)\n",
    "\n",
    "#     # Load CSV\n",
    "#     df = pd.read_csv(input_csv_path)\n",
    "#     # DEBUG: run only first 20 rows\n",
    "#     # df = df.head(8)\n",
    "\n",
    "#     required_cols = {\"Question\", \"model_response\"}\n",
    "#     missing = required_cols - set(df.columns)\n",
    "#     if missing:\n",
    "#         raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "#     # Run self-rag\n",
    "#     results = []\n",
    "\n",
    "#     BATCH_SIZE = 8  # Safe for Gemma-2B / Sarvam-1 on T4\n",
    "\n",
    "#     questions = df[\"Question\"].tolist()\n",
    "#     initial_answers = df[\"model_response\"].tolist()\n",
    "\n",
    "#     # Step 2: retrieve contexts (still row-wise, cheap)\n",
    "#     contexts = []\n",
    "#     for q in tqdm(questions, desc=\"Retrieving Wikipedia\"):\n",
    "#         contexts.append(pipeline.retriever.retrieve(q, language))\n",
    "#     print(\"✅ Retrieval done. Starting BATCHED regeneration...\")\n",
    "\n",
    "#     # Step 3: BATCHED regeneration (THIS is the big win)\n",
    "#     regenerated_answers = pipeline.llm.regenerate_answer_batch(\n",
    "#         questions,\n",
    "#         initial_answers,\n",
    "#         contexts,\n",
    "#         language,\n",
    "#         batch_size=BATCH_SIZE\n",
    "#     )\n",
    "#     print(\"✅ Regeneration done. Starting critique/projection...\")\n",
    "#     print(regenerated_answers)\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     # Remaining steps (translation + critique) stay unchanged\n",
    "#     for q, ctx, regen in tqdm(\n",
    "#         zip(questions, contexts, regenerated_answers),\n",
    "#         total=len(questions),\n",
    "#         desc=\"Critique + Projection\"\n",
    "#     ):\n",
    "#         # q_en = pipeline.translator.translate_to_english(q, language)\n",
    "#         # ctx_en = pipeline.translator.translate_to_english(ctx, language)\n",
    "#         # ans_en = pipeline.translator.translate_to_english(regen, language)\n",
    "\n",
    "#         # revised_answer_en = pipeline.llm.critique_answer(\n",
    "#         #     q_en, ctx_en, ans_en\n",
    "#         # )\n",
    "\n",
    "#         # final_L = pipeline.translator.translate_from_english(\n",
    "#         #     revised_answer_en, language\n",
    "#         # )\n",
    "\n",
    "#         results.append({\n",
    "#             \"final_answer\": regen\n",
    "#         })\n",
    "\n",
    "\n",
    "#     # Add results to dataframe\n",
    "#     for key in results[0].keys():\n",
    "#         df[f\"selfrag_{key}\"] = [r[key] for r in results]\n",
    "\n",
    "#     # Generate output path\n",
    "#     output_csv_path = input_csv_path.replace(\n",
    "#         os.sep + \"Vanilla\" + os.sep,\n",
    "#         os.sep + \"selfrag\" + os.sep\n",
    "#     )\n",
    "\n",
    "#     os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "\n",
    "#     # Save results\n",
    "#     df.to_csv(output_csv_path, index=False)\n",
    "#     print(f\"\\n✅ Results saved to: {output_csv_path}\")\n",
    "\n",
    "#     # Cleanup\n",
    "#     pipeline.cleanup()\n",
    "\n",
    "#     return output_csv_path\n",
    "\n",
    "def run_selfrag_on_csv(\n",
    "    input_csv_path,\n",
    "    model_type=None,\n",
    "    language=None,\n",
    "    retrieval_cache_path=\"/content/drive/MyDrive/Project/wikipedia_retrieval_cache.db\",\n",
    "    shared_retrieval_pipeline=None,   # optional: reuse one pipeline for retrieval\n",
    "):\n",
    "    # Auto-detect\n",
    "    if model_type is None or language is None:\n",
    "        detected_model, detected_language = detect_model_and_language_from_path(input_csv_path)\n",
    "        model_type = detected_model if model_type is None else model_type\n",
    "        language = detected_language if language is None else language\n",
    "\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"Processing: {os.path.basename(input_csv_path)}\")\n",
    "    print(f\"Model: {model_type.upper()}\")\n",
    "    print(f\"Language: {language.upper()}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Build model-specific pipeline (for regeneration)\n",
    "    config = Config()\n",
    "    config.model_type = model_type\n",
    "    if model_type == \"aya\":\n",
    "        config.load_in_4bit = True\n",
    "\n",
    "    pipeline = CrossLingualSelfRAG(config)\n",
    "\n",
    "    df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    required_cols = {\"Question\", \"model_response\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    questions = df[\"Question\"].tolist()\n",
    "    initial_answers = df[\"model_response\"].tolist()\n",
    "\n",
    "    # Use cache\n",
    "    cache = RetrievalCache(retrieval_cache_path)\n",
    "\n",
    "    # Use a shared pipeline for retrieval if provided (so you don't instantiate twice)\n",
    "    retrieval_pipeline = shared_retrieval_pipeline if shared_retrieval_pipeline is not None else pipeline\n",
    "\n",
    "    contexts = []\n",
    "    misses = 0\n",
    "    for q in tqdm(questions, desc=\"Retrieving Wikipedia (cached)\"):\n",
    "        cached_ctx = cache.get(q, language)\n",
    "        if cached_ctx is None:\n",
    "            ctx = retrieval_pipeline.retriever.retrieve(q, language)\n",
    "            cache.set(q, language, ctx)\n",
    "            misses += 1\n",
    "        else:\n",
    "            ctx = cached_ctx\n",
    "        contexts.append(ctx)\n",
    "\n",
    "    print(f\"✅ Retrieval done. Cache misses: {misses}/{len(questions)}. Starting BATCHED regeneration...\")\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    regenerated_answers = pipeline.llm.regenerate_answer_batch(\n",
    "        questions,\n",
    "        initial_answers,\n",
    "        contexts,\n",
    "        language,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    results = [{\"final_answer\": regen} for regen in regenerated_answers]\n",
    "\n",
    "    for key in results[0].keys():\n",
    "        df[f\"selfrag_{key}\"] = [r[key] for r in results]\n",
    "\n",
    "    output_csv_path = input_csv_path.replace(\n",
    "        os.sep + \"Vanilla\" + os.sep,\n",
    "        os.sep + \"selfrag\" + os.sep\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(output_csv_path), exist_ok=True)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"\\n✅ Results saved to: {output_csv_path}\")\n",
    "\n",
    "    pipeline.cleanup()\n",
    "    return output_csv_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WAI0UNjAdWq"
   },
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2734,
     "status": "ok",
     "timestamp": 1765970141950,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "D-g03brg35R6",
    "outputId": "5146c36c-1cf5-4baf-842a-87741c805284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576,
     "referenced_widgets": [
      "ecbf0514dcb64a51b432e3c3950eb31f",
      "a86a9dce3b324ccea29c4e78aae46505",
      "df7b5bc493f94ea4b6ccb706d54c7a87",
      "9258deadaffd4d678c212bf1e3a0f251",
      "243fd6ac002c4bad873b945ca026622e",
      "2179f228333f4c4c81cd68b14500c317",
      "bf3c7bd641394500bfd0c4b65e27b71d",
      "ca0bf5d7ff1149c6a86503b5daaa6a57",
      "2caf82c30b1e4ddbbe0f54c97a23ede4",
      "a1b63ffc81c4474b8239d52433520ba8",
      "8ff61f267f7a4d21b51f03fb599ccc19"
     ]
    },
    "executionInfo": {
     "elapsed": 72664,
     "status": "ok",
     "timestamp": 1765970214619,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "8n3NFAfOG17k",
    "outputId": "8304c1ac-afe1-415b-b215-a955363bc6b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: sarvam_results_en.csv\n",
      "Model: GEMMA\n",
      "Language: ENGLISH\n",
      "============================================================\n",
      "\n",
      "Initializing Cross-Lingual Self-RAG modules...\n",
      "============================================================\n",
      "1. Initializing Wikipedia Retriever...\n",
      "\n",
      "2. Initializing Translation Module...\n",
      "\n",
      "3. Initializing GEMMA Model...\n",
      "Loading GEMMA model: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbf0514dcb64a51b432e3c3950eb31f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GEMMA Model loaded successfully!\n",
      "Model parameters: 2.51B\n",
      "============================================================\n",
      "✓ All modules initialized successfully!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving Wikipedia (cached): 100%|██████████| 200/200 [00:02<00:00, 96.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieval done. Cache misses: 0/200. Starting BATCHED regeneration...\n",
      "\n",
      "✅ Results saved to: /content/drive/MyDrive/Project/Dataset/IndicQuest/Results/selfrag/sarvam/sarvam_results_en.csv\n",
      "\n",
      "Cleaning up resources...\n",
      "✓ Cleanup complete\n",
      "Saved to: /content/drive/MyDrive/Project/Dataset/IndicQuest/Results/selfrag/sarvam/sarvam_results_en.csv\n"
     ]
    }
   ],
   "source": [
    "  # Example 1: Process a single CSV file (auto-detect model and language)\n",
    "input_csv = \"/content/drive/MyDrive/Project/Dataset/IndicQuest/Results/Vanilla/sarvam/sarvam_results_en.csv\"\n",
    "output_csv = run_selfrag_on_csv(input_csv)\n",
    "print(\"Saved to:\", output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558,
     "referenced_widgets": [
      "feb370756be8421ab8ba333c01c23594",
      "f6a049fb28f543a4a291969daefba665",
      "a8316e0a9dbe47f1ac6293d459c75429",
      "9fc546e7cd4844af88531f214699eda7",
      "a5836319506a4a029cb0d955dc44cf8b",
      "8a789ff87f91462583ff441932b0fe80",
      "422e1fe1720d4652827920d52a0cafe7",
      "f14415ee7a214341b02414c40eeff6b7",
      "a422625c52eb4061a94c0f1e637f3f79",
      "1e9f5addc2854b5dbdd4e7cf32023d74",
      "7d064eac3e284ddf98822927b9bddb52"
     ]
    },
    "executionInfo": {
     "elapsed": 79045,
     "status": "ok",
     "timestamp": 1765970293670,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "pUpd6ujK5o1h",
    "outputId": "8c5c6d01-f76b-4b71-9377-6d0f012797d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: gemma_results_en.csv\n",
      "Model: GEMMA\n",
      "Language: ENGLISH\n",
      "============================================================\n",
      "\n",
      "Initializing Cross-Lingual Self-RAG modules...\n",
      "============================================================\n",
      "1. Initializing Wikipedia Retriever...\n",
      "\n",
      "2. Initializing Translation Module...\n",
      "\n",
      "3. Initializing GEMMA Model...\n",
      "Loading GEMMA model: google/gemma-2b-it\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb370756be8421ab8ba333c01c23594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GEMMA Model loaded successfully!\n",
      "Model parameters: 2.51B\n",
      "============================================================\n",
      "✓ All modules initialized successfully!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving Wikipedia (cached): 100%|██████████| 200/200 [00:01<00:00, 116.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Retrieval done. Cache misses: 0/200. Starting BATCHED regeneration...\n",
      "\n",
      "✅ Results saved to: /content/drive/MyDrive/Project/Dataset/IndicQuest/Results/selfrag/gemma/gemma_results_en.csv\n",
      "\n",
      "Cleaning up resources...\n",
      "✓ Cleanup complete\n",
      "Saved to: /content/drive/MyDrive/Project/Dataset/IndicQuest/Results/selfrag/gemma/gemma_results_en.csv\n"
     ]
    }
   ],
   "source": [
    "input_csv = \"/content/drive/MyDrive/Project/Dataset/IndicQuest/Results/Vanilla/gemma/gemma_results_en.csv\"\n",
    "output_csv = run_selfrag_on_csv(input_csv)\n",
    "print(\"Saved to:\", output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvlsCTha1RpK"
   },
   "source": [
    "## 11. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "5d1d3edeb0c643c2beac6ad7e8334045",
      "afd1ada85906475c9c0423abb229eccf",
      "5c50a307b98a4881a096938609832ecb",
      "091720b5fc174878a33097572fd15700",
      "c3e586abe54643299c06d50044010e65",
      "653129671b8a42cd952f9534dbc6ae8c",
      "3953daf43aaa4d59af6a1e401df8bffe",
      "5182a4875fab46d195665a54092f0758",
      "cee4d959f2ef4e32b31487c2a94d5c52",
      "111cc7878d184b70a1ba699586768556",
      "d088a397c45047f7836659fd7cf8736b"
     ]
    },
    "executionInfo": {
     "elapsed": 963375,
     "status": "error",
     "timestamp": 1765734356640,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "mOYcwpVx1RpK",
    "outputId": "c61089bc-5902-47b0-e35d-25be95e18723"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: sarvam_results_ta.csv\n",
      "Model: SARVAM\n",
      "Language: TAMIL\n",
      "============================================================\n",
      "\n",
      "Initializing Cross-Lingual Self-RAG modules...\n",
      "============================================================\n",
      "1. Initializing Wikipedia Retriever...\n",
      "\n",
      "2. Initializing Translation Module...\n",
      "\n",
      "3. Initializing SARVAM Model...\n",
      "Loading SARVAM model: sarvamai/sarvam-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d1d3edeb0c643c2beac6ad7e8334045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SARVAM Model loaded successfully!\n",
      "Model parameters: 2.53B\n",
      "============================================================\n",
      "✓ All modules initialized successfully!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['1920ல் ஒத்துழையாமை இயக்கம் தொடங்கப்பட்டபோது யாருடைய மரணம் நிகழ்ந்தது', '1920ல் ஒத்துழையாமை இயக்கம் தொடங்கப்பட்டபோது யாருடைய', '1920ல் ஒத்துழையாமை', 'ஒத்துழையாமை இயக்கம்', 'இயக்கம் தொடங்கப்பட்டபோது', 'தொடங்கப்பட்டபோது யாருடைய', '1920ல்', 'ஒத்துழையாமை', 'இயக்கம்', 'தொடங்கப்பட்டபோது']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported/Partial Support/Not Supported\n",
      "\n",
      "• The judge has determined that there were sufficient reasons supporting his decision based upon the facts presented within the case study. He ...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   0%|          | 1/200 [00:27<1:30:38, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['லோகித்வாடி என்று பிரபலமாக அறியப்பட்ட இந்திய ஆர்வலர் யார்', 'லோகித்வாடி பிரபலமாக அறியப்பட்ட இந்திய ஆர்வலர்', 'லோகித்வாடி பிரபலமாக', 'பிரபலமாக அறியப்பட்ட', 'அறியப்பட்ட இந்திய', 'இந்திய ஆர்வலர்', 'லோகித்வாடி', 'பிரபலமாக', 'அறியப்பட்ட', 'இந்திய']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported/PartiallySupported/NotSupported\n",
      "\n",
      "This means that you either support it completely (\"Supported\") partially (\"PartiallySupported\"), or not at all (\"NotSupported\"). You can also...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   1%|          | 2/200 [00:52<1:26:37, 26.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['இந்தியாவின் முதல் ஜனாதிபதி மற்றும் அமெரிக்காவின் முதல் ஜனாதிபதி யார்', 'இந்தியாவின் முதல் ஜனாதிபதி மற்றும் அமெரிக்காவின்', 'இந்தியாவின் முதல்', 'முதல் ஜனாதிபதி', 'ஜனாதிபதி மற்றும்', 'மற்றும் அமெரிக்காவின்', 'இந்தியாவின்', 'முதல்', 'ஜனாதிபதி', 'மற்றும்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting ',' delimiter: line 1 column 197 (char 196)\n",
      "Raw response: {\"judgment\": \"Partially Supported\", \"rationale\": \"The question asks if it can be said that 'the person named [name], who lived between two dates', had something done with regard to another date.\" The ...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   2%|▏         | 3/200 [01:22<1:31:20, 27.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['அப்துல் கலாம் யார் சுருக்கமாக விவரிக்க', 'அப்துல் கலாம் சுருக்கமாக விவரிக்க', 'அப்துல் கலாம்', 'கலாம் சுருக்கமாக', 'சுருக்கமாக விவரிக்க', 'அப்துல்', 'கலாம்', 'சுருக்கமாக', 'விவரிக்க']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 1 column 117 (char 116)\n",
      "Raw response: ```json\n",
      "{\"judgement\": \"supported\", \"rationale\": \"\\\\\\\"I think it supports my argument because...\\\\\\\" \\\\\\\"This quote says...\"}, {\"confidence\": .8}\n",
      "```\n",
      "or `\"\"`, if there isn't enough information availabl...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   2%|▏         | 4/200 [01:41<1:19:01, 24.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['மோடியின் முதல் ஆட்சிக் காலத்தில் இந்தியாவின் நிதியமைச்சராக இருந்தவர் யார்', 'மோடியின் முதல் ஆட்சிக் காலத்தில் இந்தியாவின்', 'மோடியின் முதல்', 'முதல் ஆட்சிக்', 'ஆட்சிக் காலத்தில்', 'காலத்தில் இந்தியாவின்', 'மோடியின்', 'முதல்', 'ஆட்சிக்', 'காலத்தில்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 1 column 378 (char 377)\n",
      "Raw response: `{\"judgement\": \"supported\", \"rationale\": \"\\n\\tThis question asks about \\\"the role\\\" that religion plays within society.\\n\\tThe most relevant example would be religious institutions providing social se...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   2%|▎         | 5/200 [02:07<1:21:25, 25.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['நரேந்திர மோடியின் கல்வித் தகுதி என்ன', 'நரேந்திர மோடியின் கல்வித் தகுதி', 'நரேந்திர மோடியின்', 'மோடியின் கல்வித்', 'கல்வித் தகுதி', 'நரேந்திர', 'மோடியின்', 'கல்வித்', 'தகுதி']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   3%|▎         | 6/200 [02:38<1:27:49, 27.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['அயோத்தியில் சர்ச்சைக்குரிய கட்டிடம் எப்போது இடிக்கப்பட்டது மாநிலம் எப்படி இருந்தது அரசு தண்டிக்கப்பட்டதா', 'அயோத்தியில் சர்ச்சைக்குரிய கட்டிடம் இடிக்கப்பட்டது மாநிலம்', 'அயோத்தியில் சர்ச்சைக்குரிய', 'சர்ச்சைக்குரிய கட்டிடம்', 'கட்டிடம் இடிக்கப்பட்டது', 'இடிக்கப்பட்டது மாநிலம்', 'அயோத்தியில்', 'சர்ச்சைக்குரிய', 'கட்டிடம்', 'இடிக்கப்பட்டது']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement and rationale for each question should be written as one sentence using only lowercase letters without any special characters such as !@$%^&*()+=[]\\|/., etc... The judgment will always ...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   4%|▎         | 7/200 [03:04<1:25:31, 26.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['இந்தியக் குடியரசுத் தலைவரைத் தேர்ந்தெடுக்கும் அரசியல் சட்டப் பிரிவு எது', 'இந்தியக் குடியரசுத் தலைவரைத் தேர்ந்தெடுக்கும் அரசியல்', 'இந்தியக் குடியரசுத்', 'குடியரசுத் தலைவரைத்', 'தலைவரைத் தேர்ந்தெடுக்கும்', 'தேர்ந்தெடுக்கும் அரசியல்', 'இந்தியக்', 'குடியரசுத்', 'தலைவரைத்', 'தேர்ந்தெடுக்கும்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 1 column 423 (char 422)\n",
      "Raw response: ```json\n",
      "{\"judgment\": \"supported\", \"rationale\": \"\\n\\nIndia's President election process can be found here \\nhttps://www.indianpoliticsforums.com/threads/president-of-india-electoral-process.3655/page-2...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   4%|▍         | 8/200 [03:35<1:29:36, 28.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['மகாராஷ்டிராவில் உள்ள மக்களவைத் தொகுதிகளின் எண்ணிக்கை என்ன', 'மகாராஷ்டிராவில் உள்ள மக்களவைத் தொகுதிகளின் எண்ணிக்கை', 'மகாராஷ்டிராவில் உள்ள', 'உள்ள மக்களவைத்', 'மக்களவைத் தொகுதிகளின்', 'தொகுதிகளின் எண்ணிக்கை', 'மகாராஷ்டிராவில்', 'உள்ள', 'மக்களவைத்', 'தொகுதிகளின்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported/Partial Support/Not Supported\n",
      "\n",
      "The judge has decided that it would be best if we did not do X because Y happened after Z occurred. The reason why they think so can be found h...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   4%|▍         | 9/200 [04:00<1:26:42, 27.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['அரசியலமைப்பின் வரைவுக் குழுவின் முன் முன்னுரையை முன்மொழிந்தவர்', 'அரசியலமைப்பின் வரைவுக் குழுவின் முன் முன்னுரையை', 'அரசியலமைப்பின் வரைவுக்', 'வரைவுக் குழுவின்', 'குழுவின் முன்', 'முன் முன்னுரையை', 'அரசியலமைப்பின்', 'வரைவுக்', 'குழுவின்', 'முன்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: ```json\n",
      "{\n",
      "  \"judgment\": \"supported\",\n",
      "  \"rationale\": \"Partial support due to strong emphasis placed on goal establishment rather than methodology selection leaving some ambiguity surrounding potential ...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   5%|▌         | 10/200 [04:49<1:46:57, 33.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['1973 இல் இந்தியாவின் தலைமை நீதிபதியாக நியமிக்கப்பட்டவர் யார் இந்த நியமனம் ஏன் சர்ச்சையானது', '1973 இல் இந்தியாவின் தலைமை நீதிபதியாக', '1973 இல்', 'இல் இந்தியாவின்', 'இந்தியாவின் தலைமை', 'தலைமை நீதிபதியாக', '1973', 'இல்', 'இந்தியாவின்', 'தலைமை']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   6%|▌         | 11/200 [05:06<1:30:29, 28.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['மார்ச் 1977 பொதுத் தேர்தலில் ஜனதா கட்சியும் அதன் கூட்டணிக் கட்சிகளும் எத்தனை மக்களவைத் தொகுதிகளை வென்றன', 'மார்ச் 1977 பொதுத் தேர்தலில் ஜனதா', 'மார்ச் 1977', '1977 பொதுத்', 'பொதுத் தேர்தலில்', 'தேர்தலில் ஜனதா', 'மார்ச்', '1977', 'பொதுத்', 'தேர்தலில்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 3 column 1 (char 670)\n",
      "Raw response: {\"judgment\": \"supported\", \"rationale\": \"\\n\\tThis question asks about \\u201cthe role that government plays.\\u201d The correct answer would be something like \\u201cGovernments play important roles such ...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   6%|▌         | 12/200 [05:24<1:19:21, 25.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['2024 இன் படி மகாராஷ்டிராவின் முதல்வர் யார்', '2024 இன் படி மகாராஷ்டிராவின் முதல்வர்', '2024 இன்', 'இன் படி', 'படி மகாராஷ்டிராவின்', 'மகாராஷ்டிராவின் முதல்வர்', '2024', 'இன்', 'படி', 'மகாராஷ்டிராவின்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: This task requires you to analyze and evaluate arguments based on their content within its original context. You must use only this information for evaluation purposes - no external knowledge should b...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   6%|▋         | 13/200 [05:50<1:19:45, 25.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['1975 ஆம் ஆண்டு ஜூன் 12 ஆம் தேதி பிரதமர் இந்திரா காந்திக்கு எதிராக எந்த உயர்நீதிமன்றம் தீர்ப்பு வழங்கியது மேலும் அவர் 1971 ஆம் ஆண்டு மக்களவை உறுப்பினர் பதவியை இழந்தார்', '1975 ஆம் ஆண்டு ஜூன் 12', '1975 ஆம்', 'ஆம் ஆண்டு', 'ஆண்டு ஜூன்', 'ஜூன் 12', '1975', 'ஆம்', 'ஆண்டு', 'ஜூன்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   7%|▋         | 14/200 [05:59<1:03:39, 20.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['இந்தியாவில் சமஸ்தானங்களை ஒன்றிணைப்பதில் சர்தார் படேல் ஆற்றிய பங்கை விளக்குங்கள்', 'இந்தியாவில் சமஸ்தானங்களை ஒன்றிணைப்பதில் சர்தார் படேல்', 'இந்தியாவில் சமஸ்தானங்களை', 'சமஸ்தானங்களை ஒன்றிணைப்பதில்', 'ஒன்றிணைப்பதில் சர்தார்', 'சர்தார் படேல்', 'இந்தியாவில்', 'சமஸ்தானங்களை', 'ஒன்றிணைப்பதில்', 'சர்தார்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting ',' delimiter: line 1 column 90 (char 89)\n",
      "Raw response: ```json\n",
      "{\"judgment\": \"supported\", \"rationale\": \"\\\\\\\"This response provides support based on \\\\\\\\\"context\\\\\\\\\\\".\\\\nIt does not offer new information beyond existing knowledge.\", \"revised_answer\": \"Caut...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   8%|▊         | 15/200 [06:33<1:16:22, 24.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['1952 முதல் பொதுத் தேர்தலில் காங்கிரஸ் கட்சி மற்றும் பாரதிய ஜனசங்கத்தின் தேர்தல் சின்னங்கள் என்ன', '1952 முதல் பொதுத் தேர்தலில் காங்கிரஸ்', '1952 முதல்', 'முதல் பொதுத்', 'பொதுத் தேர்தலில்', 'தேர்தலில் காங்கிரஸ்', '1952', 'முதல்', 'பொதுத்', 'தேர்தலில்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 2 column 1 (char 71)\n",
      "Raw response: ```json\n",
      "{\"judgment\": \"supported\", \"rationale\": \"...\", \"revised_answer\": \"...\"}\n",
      "```\n",
      "\n",
      "Example response with `judgement`, `rationale`, and `revised_answer` fields filled out:\n",
      "```json\n",
      "{\"judgment\": \"suppor...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   8%|▊         | 16/200 [06:44<1:02:46, 20.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['சிவசேனாவின் தலைமையகம் எங்கே', 'சிவசேனாவின் தலைமையகம்', 'சிவசேனாவின்', 'தலைமையகம்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)\n",
      "Raw response: #### Judgement: Supported/PartiallySupported/NotSupported\n",
      "\n",
      "#### Rationale: Brief Explanation Citing Specific Evidence From Context\n",
      "\n",
      "#### Revised Answer: Cautionary Strictly Based On Context\n",
      "\n",
      "#### Conf...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   8%|▊         | 17/200 [07:09<1:06:20, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['சொந்தமாக அதிகாரப்பூர்வ கொடியை வைத்திருந்த ஒரே இந்திய மாநிலம் எது', 'சொந்தமாக அதிகாரப்பூர்வ கொடியை வைத்திருந்த ஒரே', 'சொந்தமாக அதிகாரப்பூர்வ', 'அதிகாரப்பூர்வ கொடியை', 'கொடியை வைத்திருந்த', 'வைத்திருந்த ஒரே', 'சொந்தமாக', 'அதிகாரப்பூர்வ', 'கொடியை', 'வைத்திருந்த']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported\n",
      "\n",
      "This task was partially successful because I could not find any relevant information on this topic outside of my personal knowledge base. However, if you were able to locate...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   9%|▉         | 18/200 [07:35<1:10:33, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['பஞ்சாபில் ஆதிக்கம் செலுத்தும் மூன்று அரசியல் கட்சிகளை பட்டியலிடுங்கள்', 'பஞ்சாபில் ஆதிக்கம் செலுத்தும் மூன்று அரசியல்', 'பஞ்சாபில் ஆதிக்கம்', 'ஆதிக்கம் செலுத்தும்', 'செலுத்தும் மூன்று', 'மூன்று அரசியல்', 'பஞ்சாபில்', 'ஆதிக்கம்', 'செலுத்தும்', 'மூன்று']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  10%|▉         | 19/200 [07:43<55:59, 18.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['2024 இல் உ பி யின் சுகாதார அமைச்சர் யார்', '2024 இல் உபியின் சுகாதார அமைச்சர்', '2024 இல்', 'இல் உபியின்', 'உபியின் சுகாதார', 'சுகாதார அமைச்சர்', '2024', 'இல்', 'உபியின்', 'சுகாதார']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 1 column 139 (char 138)\n",
      "Raw response: {\"judgment\": \"supported\", \"rationale\": \"Supporting information cited directly from context.\", \"revised_answer\": \"This should not change.\"}, {\"judgment\": \"partially_supported\", \"rationale\": \"Partial su...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  10%|█         | 20/200 [08:15<1:07:40, 22.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['2024 ஆம் ஆண்டு வரை எந்த அரசாங்கம் தற்போது MP இல் ஆட்சியில் உள்ளது', '2024 ஆம் ஆண்டு வரை எந்த', '2024 ஆம்', 'ஆம் ஆண்டு', 'ஆண்டு வரை', 'வரை எந்த', '2024', 'ஆம்', 'ஆண்டு', 'வரை']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 1 column 361 (char 360)\n",
      "Raw response: {\"judgment\": \"supported\", \"rationale\": \"\\n\\tThis question asks about \\u201cthe role that social media plays in political campaigns.\\u201d The most relevant information comes directly from our source m...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  10%|█         | 21/200 [08:36<1:06:23, 22.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['2007 ஆம் ஆண்டு உ பி சட்டமன்றத் தேர்தல்களின் கருத்துக்கணிப்பு முடிவுகள் என்ன', '2007 ஆம் ஆண்டு உபி சட்டமன்றத்', '2007 ஆம்', 'ஆம் ஆண்டு', 'ஆண்டு உபி', 'உபி சட்டமன்றத்', '2007', 'ஆம்', 'ஆண்டு', 'உபி']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  11%|█         | 22/200 [09:05<1:11:29, 24.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['முதன்முதலில் மு கருணாநிதி எந்த தொகுதியில் இருந்து தமிழ்நாடு சட்டமன்றத்திற்கு தேர்ந்தெடுக்கப்பட்டார்', 'முதன்முதலில் மு கருணாநிதி எந்த தொகுதியில்', 'முதன்முதலில் மு', 'மு கருணாநிதி', 'கருணாநிதி எந்த', 'எந்த தொகுதியில்', 'முதன்முதலில்', 'மு', 'கருணாநிதி', 'எந்த']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  12%|█▏        | 23/200 [09:34<1:15:52, 25.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['தென்னிந்தியாவின் முதல் கம்யூனிஸ்ட்டாகக் கருதப்படுபவர்', 'தென்னிந்தியாவின் முதல்', 'முதல் கம்யூனிஸ்ட்டாகக்', 'கம்யூனிஸ்ட்டாகக் கருதப்படுபவர்', 'தென்னிந்தியாவின்', 'முதல்', 'கம்யூனிஸ்ட்டாகக்', 'கருதப்படுபவர்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported/PartiallySupported/NotSupported\n",
      "\n",
      "• The judge has decided that there's enough support here - it would have been difficult if they hadn't found some supporting material! They d...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  12%|█▏        | 24/200 [09:58<1:13:52, 25.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['ஆந்திர பிரதேசத்தில் மிகவும் செல்வாக்கு மிக்க அரசியல் கட்சியை நிறுவியவர் யார்', 'ஆந்திர பிரதேசத்தில் மிகவும் செல்வாக்கு மிக்க', 'ஆந்திர பிரதேசத்தில்', 'பிரதேசத்தில் மிகவும்', 'மிகவும் செல்வாக்கு', 'செல்வாக்கு மிக்க', 'ஆந்திர', 'பிரதேசத்தில்', 'மிகவும்', 'செல்வாக்கு']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported/PartiallySupported/NotSupported\n",
      "\n",
      "The judge has decided that there were sufficient grounds for supporting my argument with respect to XYZ issue(s). The reason behind their dec...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  12%|█▎        | 25/200 [10:22<1:11:55, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['கர்நாடக அரசியலில் ஆதிக்கம் செலுத்தும் சாதிக் குழுக்களைக் குறிப்பிடுங்கள்', 'கர்நாடக அரசியலில் ஆதிக்கம் செலுத்தும் சாதிக்', 'கர்நாடக அரசியலில்', 'அரசியலில் ஆதிக்கம்', 'ஆதிக்கம் செலுத்தும்', 'செலுத்தும் சாதிக்', 'கர்நாடக', 'அரசியலில்', 'ஆதிக்கம்', 'செலுத்தும்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  13%|█▎        | 26/200 [10:37<1:03:15, 21.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['இந்தியாவில் எந்த மாநிலம் கம்யூனிஸ்டுகளை ஆட்சிக்கு தேர்ந்தெடுத்தது', 'இந்தியாவில் எந்த மாநிலம் கம்யூனிஸ்டுகளை ஆட்சிக்கு', 'இந்தியாவில் எந்த', 'எந்த மாநிலம்', 'மாநிலம் கம்யூனிஸ்டுகளை', 'கம்யூனிஸ்டுகளை ஆட்சிக்கு', 'இந்தியாவில்', 'எந்த', 'மாநிலம்', 'கம்யூனிஸ்டுகளை']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported\n",
      "\n",
      "• Rationale: A brief explanation citing specific evidence from context that supports my position; e.g., “I believe X because Y.”\n",
      "• Confidence: High confidence based on suppo...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  14%|█▎        | 27/200 [11:08<1:11:20, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['பீகாரில் நிர்வாக நோக்கங்களுக்காக எத்தனை பிரிவுகள் உள்ளன', 'பீகாரில் நிர்வாக நோக்கங்களுக்காக எத்தனை பிரிவுகள்', 'பீகாரில் நிர்வாக', 'நிர்வாக நோக்கங்களுக்காக', 'நோக்கங்களுக்காக எத்தனை', 'எத்தனை பிரிவுகள்', 'பீகாரில்', 'நிர்வாக', 'நோக்கங்களுக்காக', 'எத்தனை']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting ',' delimiter: line 1 column 136 (char 135)\n",
      "Raw response: {\"judgment\": \"Not Supported\", \"rationale\": \"There isn't any concrete info available regarding division of bihari states into districts\". The task requires you to provide only what can be inferred from...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  14%|█▍        | 28/200 [11:31<1:09:30, 24.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['மேற்கு வங்கத்தில் அகதிகளின் வருகைக்கு என்ன காரணம்', 'மேற்கு வங்கத்தில் அகதிகளின் வருகைக்கு காரணம்', 'மேற்கு வங்கத்தில்', 'வங்கத்தில் அகதிகளின்', 'அகதிகளின் வருகைக்கு', 'வருகைக்கு காரணம்', 'மேற்கு', 'வங்கத்தில்', 'அகதிகளின்', 'வருகைக்கு']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 2 column 1 (char 78)\n",
      "Raw response: ```json\n",
      "{\"judgment\": \"supported\", \"rationale\": \"\\n\\t... \\n\", \"revised_answer\": \"...\"}\n",
      "```\n",
      "\n",
      "Example output:\n",
      "```json\n",
      "{\"judgment\": \"supported\", \"rationale\": \"\\n\\t... \\n\", \"revised_answer\": \"I think I sup...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  14%|█▍        | 29/200 [12:00<1:12:29, 25.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['2024 ஆம் ஆண்டு நிலவரப்படி மேற்கு வங்காளத்தில் முதன்மையான அரசியல் கட்சி எது', '2024 ஆம் ஆண்டு நிலவரப்படி மேற்கு', '2024 ஆம்', 'ஆம் ஆண்டு', 'ஆண்டு நிலவரப்படி', 'நிலவரப்படி மேற்கு', '2024', 'ஆம்', 'ஆண்டு', 'நிலவரப்படி']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: partially_supported\n",
      "\n",
      "Rationale: The judge believes that there may have been some misunderstanding about what constitutes an acceptable use case when using AI systems like ChatGPT; howe...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  15%|█▌        | 30/200 [12:27<1:13:21, 25.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['ஒரிசாவில் ஒரு மாகாண சட்டமன்றத் தேர்தலுக்கு எந்தச் சட்டம் வழங்கப்பட்டது', 'ஒரிசாவில் ஒரு மாகாண சட்டமன்றத் தேர்தலுக்கு', 'ஒரிசாவில் ஒரு', 'ஒரு மாகாண', 'மாகாண சட்டமன்றத்', 'சட்டமன்றத் தேர்தலுக்கு', 'ஒரிசாவில்', 'ஒரு', 'மாகாண', 'சட்டமன்றத்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  16%|█▌        | 31/200 [12:56<1:16:03, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['குஜராத்தின் முதல் பெண் முதல்வர் யார்', 'குஜராத்தின் முதல் பெண் முதல்வர்', 'குஜராத்தின் முதல்', 'முதல் பெண்', 'பெண் முதல்வர்', 'குஜராத்தின்', 'முதல்', 'பெண்', 'முதல்வர்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  16%|█▌        | 32/200 [13:34<1:24:19, 30.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['2022 குஜராத் சட்டப் பேரவைத் தேர்தலில் எந்தக் கட்சி சூப்பர் மெஜாரிட்டி பெற்றது', '2022 குஜராத் சட்டப் பேரவைத் தேர்தலில்', '2022 குஜராத்', 'குஜராத் சட்டப்', 'சட்டப் பேரவைத்', 'பேரவைத் தேர்தலில்', '2022', 'குஜராத்', 'சட்டப்', 'பேரவைத்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported/PartiallySupported/NotSupported\n",
      "\n",
      "• A brief explanation citing specific evidence from context\n",
      "• Relevant information about why you think it's either one of those three options...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  16%|█▋        | 33/200 [13:57<1:18:04, 28.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['2012 கோவா தேர்தலில் இந்திய தேசிய காங்கிரசை தோற்கடித்த கட்சி எது', '2012 கோவா தேர்தலில் இந்திய தேசிய', '2012 கோவா', 'கோவா தேர்தலில்', 'தேர்தலில் இந்திய', 'இந்திய தேசிய', '2012', 'கோவா', 'தேர்தலில்', 'இந்திய']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported/Partial Support/Not Supported\n",
      "\n",
      "The judge has determined that it would have been appropriate to give partial support based upon their interpretation of the facts presented at ...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  17%|█▋        | 34/200 [14:26<1:18:19, 28.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['குஜராத்தில் மொத்தம் எத்தனை தொகுதிகள் உள்ளன', 'குஜராத்தில் மொத்தம்', 'மொத்தம் எத்தனை', 'எத்தனை தொகுதிகள்', 'தொகுதிகள் உள்ளன', 'குஜராத்தில்', 'மொத்தம்', 'எத்தனை', 'தொகுதிகள்', 'உள்ளன']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  18%|█▊        | 35/200 [14:51<1:15:30, 27.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['மகாராஷ்டிரா நவநிர்மாண் சேனாவை உருவாக்கியவர் யார்', 'மகாராஷ்டிரா நவநிர்மாண் சேனாவை உருவாக்கியவர்', 'மகாராஷ்டிரா நவநிர்மாண்', 'நவநிர்மாண் சேனாவை', 'சேனாவை உருவாக்கியவர்', 'மகாராஷ்டிரா', 'நவநிர்மாண்', 'சேனாவை', 'உருவாக்கியவர்']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  18%|█▊        | 36/200 [15:11<1:08:55, 25.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['2015 பீகார் தேர்தலில் சமூக வரலாற்றாசிரியர் பத்ரி நாராயண் அடையாளம் காட்டிய முக்கிய காரணி என்ன', '2015 பீகார் தேர்தலில் சமூக வரலாற்றாசிரியர்', '2015 பீகார்', 'பீகார் தேர்தலில்', 'தேர்தலில் சமூக', 'சமூக வரலாற்றாசிரியர்', '2015', 'பீகார்', 'தேர்தலில்', 'சமூக']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Extra data: line 2 column 1 (char 72)\n",
      "Raw response: ```json\n",
      "{\"judgement\": \"supported\", \"rationale\": \"...\", \"revised_answer\": \"...\"}\n",
      "```\n",
      "\n",
      "Example Answers:\n",
      "• Supported - A strong argument that supports my position with clear reasoning backed up by releva...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  18%|█▊        | 37/200 [15:37<1:08:53, 25.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['இந்திய தேசிய வளர்ச்சி உள்ளடக்கிய கூட்டணி இந்தியா உருவாவதற்கான உந்துதல் என்ன', 'இந்திய தேசிய வளர்ச்சி உள்ளடக்கிய கூட்டணி', 'இந்திய தேசிய', 'தேசிய வளர்ச்சி', 'வளர்ச்சி உள்ளடக்கிய', 'உள்ளடக்கிய கூட்டணி', 'இந்திய', 'தேசிய', 'வளர்ச்சி', 'உள்ளடக்கிய']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:  18%|█▊        | 37/200 [15:40<1:09:03, 25.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-141955011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 1: Process a single CSV file (auto-detect model and language)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minput_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Project/Dataset/IndicQuest/Results/Vanilla/sarvam/sarvam_results_ta.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moutput_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_selfrag_on_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved to:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_csv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2335797899.py\u001b[0m in \u001b[0;36mrun_selfrag_on_csv\u001b[0;34m(input_csv_path, model_type, language)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             result = pipeline.process(\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mquestion_L\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0manswer_L\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_answer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-322131710.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, question_L, answer_L, language_L, context_L)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Step 3: Self-RAG Regeneration in source language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         regenerated_answer_L = self.llm.regenerate_answer(\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0mquestion_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_L\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage_L\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         )\n",
      "\u001b[0;32m/tmp/ipython-input-977714476.py\u001b[0m in \u001b[0;36mregenerate_answer\u001b[0;34m(self, question, initial_answer, context, language)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mformatted_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mregenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-977714476.py\u001b[0m in \u001b[0;36m_generate_response\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Generate response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 outputs = self.model.generate(\n\u001b[0m\u001b[1;32m    129\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_max_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         hidden_states, _ = self.self_attn(\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposition_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_rotary_pos_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_key_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mcos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsqueeze_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0msin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munsqueeze_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mq_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mk_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mq_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mrotate_half\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;34m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example 1: Process a single CSV file (auto-detect model and language)\n",
    "input_csv = \"/content/drive/MyDrive/Project/Dataset/IndicQuest/Results/Vanilla/sarvam/sarvam_results_ta.csv\"\n",
    "output_csv = run_selfrag_on_csv(input_csv)\n",
    "print(\"Saved to:\", output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5lXAMMa1RpL"
   },
   "outputs": [],
   "source": [
    "# Example 2: Process a single CSV file with explicit model specification\n",
    "input_csv = \"/content/drive/MyDrive/Project/Dataset/IndicQuest/Results/Vanilla/sarvam/sarvam_results_Hindi.csv\"\n",
    "output_csv = run_selfrag_on_csv(input_csv, model_type=\"sarvam\", language=\"hindi\")\n",
    "print(\"Saved to:\", output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "963a8bf2985d423ab4234a08ca0297b4",
      "73407c50213d449faf3a8807e79dac32",
      "c5a195cae021488a994379df24533a5d",
      "11d78d4052484d5ea49c8c5f620e499b",
      "d9ed983aa04645c4a31799f5781e27d2",
      "3db6f952a1e248dd9ccb42e96af07145",
      "6bf0ebc72c2e4635a5e9d67bb7479285",
      "acb1c10ecec247a784e83f8ea639af4a",
      "d2aaa0cfaee04879bde2ff2794cc03ba",
      "b23e60e659584b7f869bef3afd782653",
      "00262998ac8f4306b9bbe12b90f7b939"
     ]
    },
    "executionInfo": {
     "elapsed": 231305,
     "status": "error",
     "timestamp": 1765734853924,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "BvlKOSpO1RpL",
    "outputId": "caf551d1-e20f-44d8-fc4e-5124c4d3d1a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing: SARVAM - TAMIL\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing: sarvam_results_ta.csv\n",
      "Model: SARVAM\n",
      "Language: TAMIL\n",
      "============================================================\n",
      "\n",
      "Initializing Cross-Lingual Self-RAG modules...\n",
      "============================================================\n",
      "1. Initializing Wikipedia Retriever...\n",
      "\n",
      "2. Initializing Translation Module...\n",
      "\n",
      "3. Initializing SARVAM Model...\n",
      "Loading SARVAM model: sarvamai/sarvam-1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963a8bf2985d423ab4234a08ca0297b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SARVAM Model loaded successfully!\n",
      "Model parameters: 2.53B\n",
      "============================================================\n",
      "✓ All modules initialized successfully!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['1920ல் ஒத்துழையாமை இயக்கம் தொடங்கப்பட்டபோது யாருடைய மரணம் நிகழ்ந்தது', '1920ல் ஒத்துழையாமை இயக்கம் தொடங்கப்பட்டபோது யாருடைய', '1920ல் ஒத்துழையாமை', 'ஒத்துழையாமை இயக்கம்', 'இயக்கம் தொடங்கப்பட்டபோது', 'தொடங்கப்பட்டபோது யாருடைய', '1920ல்', 'ஒத்துழையாமை', 'இயக்கம்', 'தொடங்கப்பட்டபோது']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported/PartiallySupported/NotSupported\n",
      "\n",
      "• The judge believes that it supports his decision because he can see why they would do something like that based off of past events with sim...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   0%|          | 1/200 [01:50<6:04:59, 110.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['லோகித்வாடி என்று பிரபலமாக அறியப்பட்ட இந்திய ஆர்வலர் யார்', 'லோகித்வாடி பிரபலமாக அறியப்பட்ட இந்திய ஆர்வலர்', 'லோகித்வாடி பிரபலமாக', 'பிரபலமாக அறியப்பட்ட', 'அறியப்பட்ட இந்திய', 'இந்திய ஆர்வலர்', 'லோகித்வாடி', 'பிரபலமாக', 'அறியப்பட்ட', 'இந்திய']\n",
      "Regenerated answer in tamil using SARVAM\n",
      "Error parsing critique response: Expecting value: line 1 column 1 (char 0)\n",
      "Raw response: #### Judgement: Supported\n",
      "\n",
      "The judge has found that there were sufficient grounds for believing that the defendant had committed fraud on behalf of his employer; however they have not been able to fin...\n",
      "✓ Processing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   1%|          | 2/200 [02:24<3:37:09, 65.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing Query in TAMIL ===\n",
      "[WikipediaRetriever] Query variants: ['இந்தியாவின் முதல் ஜனாதிபதி மற்றும் அமெரிக்காவின் முதல் ஜனாதிபதி யார்', 'இந்தியாவின் முதல் ஜனாதிபதி மற்றும் அமெரிக்காவின்', 'இந்தியாவின் முதல்', 'முதல் ஜனாதிபதி', 'ஜனாதிபதி மற்றும்', 'மற்றும் அமெரிக்காவின்', 'இந்தியாவின்', 'முதல்', 'ஜனாதிபதி', 'மற்றும்']\n",
      "Regenerated answer in tamil using SARVAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rProcessing rows:   1%|          | 2/200 [03:34<5:53:06, 107.00s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-404707027.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example 3: Process all Sarvam model files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Project/Dataset/IndicQuest\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m processed, failed = process_all_csv_files(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sarvam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4208044864.py\u001b[0m in \u001b[0;36mprocess_all_csv_files\u001b[0;34m(base_dir, models, languages)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{'='*60}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 output_path = run_selfrag_on_csv(\n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0minput_csv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2335797899.py\u001b[0m in \u001b[0;36mrun_selfrag_on_csv\u001b[0;34m(input_csv_path, model_type, language)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             result = pipeline.process(\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mquestion_L\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0manswer_L\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_answer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-322131710.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, question_L, answer_L, language_L, context_L)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Step 5: Cross-lingual self-critique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mcritique\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritique_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_EN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_EN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregenerated_answer_EN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Step 7: Back-projection to source language\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-977714476.py\u001b[0m in \u001b[0;36mcritique_answer\u001b[0;34m(self, question_en, context_en, answer_en)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mformatted_prompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Context:\\n{context_en}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-977714476.py\u001b[0m in \u001b[0;36m_generate_response\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# Generate response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 outputs = self.model.generate(\n\u001b[0m\u001b[1;32m    129\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_max_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2785\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2789\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REMOVED_TOKEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         ```\"\"\"\n\u001b[0;32m--> 459\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m                 \u001b[0;31m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdecoder_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             hidden_states = decoder_layer(\n\u001b[0m\u001b[1;32m    396\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REMOVED_TOKEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REMOVED_TOKEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REMOVED_TOKEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_REMOVED_TOKEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtied_pointers_to_remove\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 set_module_tensor_to_device(\n\u001b[0m\u001b[1;32m    361\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map, non_blocking, clear_cache)\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example 3: Process all Sarvam model files\n",
    "base_dir = \"/content/drive/MyDrive/Project/Dataset/IndicQuest\"\n",
    "processed, failed = process_all_csv_files(\n",
    "    base_dir,\n",
    "    models=['sarvam'],\n",
    "    languages=['tamil', 'hindi', 'kannada', 'marathi', 'english']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xNiAwSWX1RpL"
   },
   "outputs": [],
   "source": [
    "# Example 4: Process all Aya model files\n",
    "base_dir = \"/content/drive/MyDrive/Project/Dataset/IndicQuest\"\n",
    "processed, failed = process_all_csv_files(\n",
    "    base_dir,\n",
    "    models=['aya'],\n",
    "    languages=['tamil', 'hindi', 'kannada', 'marathi', 'english']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0vkVcTB1RpL"
   },
   "outputs": [],
   "source": [
    "# Example 5: Process both Sarvam and Aya models for all languages\n",
    "base_dir = \"/content/drive/MyDrive/Project/Dataset/IndicQuest\"\n",
    "processed, failed = process_all_csv_files(\n",
    "    base_dir,\n",
    "    models=['sarvam', 'aya'],  # Process both models\n",
    "    languages=['tamil', 'hindi', 'kannada', 'marathi', 'english']  # All languages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "executionInfo": {
     "elapsed": 1849,
     "status": "ok",
     "timestamp": 1766031689016,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "URuGdkdgn5C5",
    "outputId": "bac05851-6961-4afa-f38d-a71165e96179"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- HYPOTHESIS 1: Is English better than Indic? ---\n",
      "\n",
      "Method: Vanilla\n",
      "Mean (En): 35.17% | Mean (Indic): 37.92%\n",
      "P-Value: 0.2130 -> Not Significant\n",
      "\n",
      "Method: Selfrag\n",
      "Mean (En): 12.50% | Mean (Indic): 18.54%\n",
      "P-Value: 0.0056 -> SIGNIFICANT\n",
      "\n",
      "--- HYPOTHESIS 2: Does SelfRAG reduce Hallucinations? ---\n",
      "Mean Drop: 20.03%\n",
      "P-Value: 0.00032 -> SIGNIFICANT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# 1. Load Data\n",
    "# Ensure your CSV file is in the current directory\n",
    "df = pd.read_csv('hallucination_analysis_results_IndicQuest.csv')\n",
    "\n",
    "# Select relevant columns\n",
    "norm_cols = [\n",
    "    'Model', 'Language',\n",
    "    'Normalized_Hallucination_Percentage_Vanilla',\n",
    "    'Normalized_Hallucination_Percentage_Selfrag'\n",
    "]\n",
    "df_analysis = df[norm_cols]\n",
    "\n",
    "# --- PART A: VISUALIZATION ---\n",
    "\n",
    "# Reshape data for plotting\n",
    "df_long = pd.melt(\n",
    "    df_analysis,\n",
    "    id_vars=['Model', 'Language'],\n",
    "    value_vars=['Normalized_Hallucination_Percentage_Vanilla', 'Normalized_Hallucination_Percentage_Selfrag'],\n",
    "    var_name='Method',\n",
    "    value_name='Hallucination_Percentage'\n",
    ")\n",
    "# Clean up labels\n",
    "df_long['Method'] = df_long['Method'].str.replace('Normalized_Hallucination_Percentage_', '').str.capitalize()\n",
    "\n",
    "# Plot: Vanilla vs SelfRAG by Language\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=df_long, x='Language', y='Hallucination_Percentage', hue='Method', palette='viridis')\n",
    "plt.title('Impact of SelfRAG on Hallucination Rates across Languages')\n",
    "plt.ylabel('Normalized Hallucination % (Lower is Better)')\n",
    "plt.xlabel('Language')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# --- PART B: STATISTICAL TESTING ---\n",
    "\n",
    "# Define groups\n",
    "indic_langs = [l for l in df['Language'].unique() if l != 'en']\n",
    "en_data = df[df['Language'] == 'en']\n",
    "indic_data = df[df['Language'].isin(indic_langs)]\n",
    "\n",
    "print(\"--- HYPOTHESIS 1: Is English better than Indic? ---\")\n",
    "for method in ['Vanilla', 'Selfrag']:\n",
    "    col = f'Normalized_Hallucination_Percentage_{method}'\n",
    "\n",
    "    # Mann-Whitney U Test (Independent samples)\n",
    "    stat, p_val = stats.mannwhitneyu(en_data[col], indic_data[col], alternative='less')\n",
    "\n",
    "    print(f\"\\nMethod: {method}\")\n",
    "    print(f\"Mean (En): {en_data[col].mean():.2f}% | Mean (Indic): {indic_data[col].mean():.2f}%\")\n",
    "    print(f\"P-Value: {p_val:.4f} -> {'SIGNIFICANT' if p_val < 0.05 else 'Not Significant'}\")\n",
    "\n",
    "print(\"\\n--- HYPOTHESIS 2: Does SelfRAG reduce Hallucinations? ---\")\n",
    "# Wilcoxon Signed-Rank Test (Paired samples)\n",
    "col_vanilla = 'Normalized_Hallucination_Percentage_Vanilla'\n",
    "col_selfrag = 'Normalized_Hallucination_Percentage_Selfrag'\n",
    "\n",
    "stat, p_val = stats.wilcoxon(df[col_vanilla], df[col_selfrag], alternative='greater')\n",
    "\n",
    "print(f\"Mean Drop: {df[col_vanilla].mean() - df[col_selfrag].mean():.2f}%\")\n",
    "print(f\"P-Value: {p_val:.5f} -> {'SIGNIFICANT' if p_val < 0.05 else 'Not Significant'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1766030688516,
     "user": {
      "displayName": "Kathiresan Palaniappan",
      "userId": "06461574775865160163"
     },
     "user_tz": 300
    },
    "id": "XzZLDdPqobjP",
    "outputId": "170c56fa-962d-4ba3-9f47-4a3510ef5c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'viz_outputs' has been saved as 'viz_outputs_results.zip'\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Folder created by the visualization script\n",
    "FOLDER_TO_SAVE = \"viz_outputs\"\n",
    "\n",
    "# Name of the zip file to be created\n",
    "ZIP_NAME = \"viz_outputs_results\"\n",
    "\n",
    "# Remove existing zip if it already exists (avoids errors)\n",
    "if os.path.exists(ZIP_NAME + \".zip\"):\n",
    "    os.remove(ZIP_NAME + \".zip\")\n",
    "\n",
    "# Create zip archive\n",
    "shutil.make_archive(ZIP_NAME, 'zip', FOLDER_TO_SAVE)\n",
    "\n",
    "print(f\"Folder '{FOLDER_TO_SAVE}' has been saved as '{ZIP_NAME}.zip'\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "3hItBZGSStwR",
    "kj-JrvSEStwS",
    "-muzBICTStwS",
    "bGOo_tDNStwT",
    "fJhQPJFAStwU",
    "RzMKxMHTStwV"
   ],
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00262998ac8f4306b9bbe12b90f7b939": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "091720b5fc174878a33097572fd15700": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_111cc7878d184b70a1ba699586768556",
      "placeholder": "​",
      "style": "IPY_MODEL_d088a397c45047f7836659fd7cf8736b",
      "value": " 2/2 [00:19&lt;00:00,  8.22s/it]"
     }
    },
    "111cc7878d184b70a1ba699586768556": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "11d78d4052484d5ea49c8c5f620e499b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b23e60e659584b7f869bef3afd782653",
      "placeholder": "​",
      "style": "IPY_MODEL_00262998ac8f4306b9bbe12b90f7b939",
      "value": " 2/2 [00:15&lt;00:00,  6.72s/it]"
     }
    },
    "1e9f5addc2854b5dbdd4e7cf32023d74": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2179f228333f4c4c81cd68b14500c317": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "243fd6ac002c4bad873b945ca026622e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2caf82c30b1e4ddbbe0f54c97a23ede4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3953daf43aaa4d59af6a1e401df8bffe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3db6f952a1e248dd9ccb42e96af07145": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "422e1fe1720d4652827920d52a0cafe7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5182a4875fab46d195665a54092f0758": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c50a307b98a4881a096938609832ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5182a4875fab46d195665a54092f0758",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cee4d959f2ef4e32b31487c2a94d5c52",
      "value": 2
     }
    },
    "5d1d3edeb0c643c2beac6ad7e8334045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_afd1ada85906475c9c0423abb229eccf",
       "IPY_MODEL_5c50a307b98a4881a096938609832ecb",
       "IPY_MODEL_091720b5fc174878a33097572fd15700"
      ],
      "layout": "IPY_MODEL_c3e586abe54643299c06d50044010e65"
     }
    },
    "653129671b8a42cd952f9534dbc6ae8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6bf0ebc72c2e4635a5e9d67bb7479285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73407c50213d449faf3a8807e79dac32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3db6f952a1e248dd9ccb42e96af07145",
      "placeholder": "​",
      "style": "IPY_MODEL_6bf0ebc72c2e4635a5e9d67bb7479285",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "7d064eac3e284ddf98822927b9bddb52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a789ff87f91462583ff441932b0fe80": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ff61f267f7a4d21b51f03fb599ccc19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9258deadaffd4d678c212bf1e3a0f251": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1b63ffc81c4474b8239d52433520ba8",
      "placeholder": "​",
      "style": "IPY_MODEL_8ff61f267f7a4d21b51f03fb599ccc19",
      "value": " 2/2 [00:01&lt;00:00,  1.53s/it]"
     }
    },
    "963a8bf2985d423ab4234a08ca0297b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_73407c50213d449faf3a8807e79dac32",
       "IPY_MODEL_c5a195cae021488a994379df24533a5d",
       "IPY_MODEL_11d78d4052484d5ea49c8c5f620e499b"
      ],
      "layout": "IPY_MODEL_d9ed983aa04645c4a31799f5781e27d2"
     }
    },
    "9fc546e7cd4844af88531f214699eda7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e9f5addc2854b5dbdd4e7cf32023d74",
      "placeholder": "​",
      "style": "IPY_MODEL_7d064eac3e284ddf98822927b9bddb52",
      "value": " 2/2 [00:01&lt;00:00,  1.45s/it]"
     }
    },
    "a1b63ffc81c4474b8239d52433520ba8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a422625c52eb4061a94c0f1e637f3f79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a5836319506a4a029cb0d955dc44cf8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8316e0a9dbe47f1ac6293d459c75429": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f14415ee7a214341b02414c40eeff6b7",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a422625c52eb4061a94c0f1e637f3f79",
      "value": 2
     }
    },
    "a86a9dce3b324ccea29c4e78aae46505": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2179f228333f4c4c81cd68b14500c317",
      "placeholder": "​",
      "style": "IPY_MODEL_bf3c7bd641394500bfd0c4b65e27b71d",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "acb1c10ecec247a784e83f8ea639af4a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "afd1ada85906475c9c0423abb229eccf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_653129671b8a42cd952f9534dbc6ae8c",
      "placeholder": "​",
      "style": "IPY_MODEL_3953daf43aaa4d59af6a1e401df8bffe",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "b23e60e659584b7f869bef3afd782653": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf3c7bd641394500bfd0c4b65e27b71d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3e586abe54643299c06d50044010e65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5a195cae021488a994379df24533a5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acb1c10ecec247a784e83f8ea639af4a",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2aaa0cfaee04879bde2ff2794cc03ba",
      "value": 2
     }
    },
    "ca0bf5d7ff1149c6a86503b5daaa6a57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cee4d959f2ef4e32b31487c2a94d5c52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d088a397c45047f7836659fd7cf8736b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d2aaa0cfaee04879bde2ff2794cc03ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9ed983aa04645c4a31799f5781e27d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df7b5bc493f94ea4b6ccb706d54c7a87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca0bf5d7ff1149c6a86503b5daaa6a57",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2caf82c30b1e4ddbbe0f54c97a23ede4",
      "value": 2
     }
    },
    "ecbf0514dcb64a51b432e3c3950eb31f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a86a9dce3b324ccea29c4e78aae46505",
       "IPY_MODEL_df7b5bc493f94ea4b6ccb706d54c7a87",
       "IPY_MODEL_9258deadaffd4d678c212bf1e3a0f251"
      ],
      "layout": "IPY_MODEL_243fd6ac002c4bad873b945ca026622e"
     }
    },
    "f14415ee7a214341b02414c40eeff6b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6a049fb28f543a4a291969daefba665": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a789ff87f91462583ff441932b0fe80",
      "placeholder": "​",
      "style": "IPY_MODEL_422e1fe1720d4652827920d52a0cafe7",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "feb370756be8421ab8ba333c01c23594": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6a049fb28f543a4a291969daefba665",
       "IPY_MODEL_a8316e0a9dbe47f1ac6293d459c75429",
       "IPY_MODEL_9fc546e7cd4844af88531f214699eda7"
      ],
      "layout": "IPY_MODEL_a5836319506a4a029cb0d955dc44cf8b"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
